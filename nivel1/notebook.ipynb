{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NIVEL 1\n",
    "## Setup\n",
    "Importo librerias necesarias para usar pyspark asi como ciertas funciones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import udf, explode\n",
    "from pyspark.sql.types import StringType, MapType, IntegerType, TimestampType, StructType, StructField\n",
    "import os\n",
    "import json\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set del JAVA_HOME para correcto funcionamiento\n",
    "cambiar por JAVA_HOME de la maquina donde se ejecuta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"JAVA_HOME\"] =  \"/usr/lib/jvm/java-11-openjdk-amd64/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inicializacion del spark contex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/07/21 16:13:11 WARN Utils: Your hostname, lab resolves to a loopback address: 127.0.1.1; using 192.168.1.22 instead (on interface wlp3s0)\n",
      "23/07/21 16:13:11 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "23/07/21 16:13:11 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"testing\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Funcion parser\n",
    "- Función usada para convertir el formato de la columna experiments a un CSV, dado que spark.read.csv no soporta el tipo MapType()\n",
    "- uso UDF para poder usar esta funcion con spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parser(value):\n",
    "    json_str = value.replace(\" \", \"\").replace(\"{\", \"{\\\"\").replace(\"}\", \"\\\"}\").replace(\"=\", \"\\\":\\\"\").replace(\",\", \"\\\",\\\"\")\n",
    "    return json.loads(json_str)\n",
    "parser_udf = udf(parser, MapType(StringType(), StringType()))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lectura del dataset\n",
    "\n",
    "- Se define la ruta del archivo csv\n",
    "- Se define la estructura de los datos. (spark la puede deducir, solo se hace por seguridad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = \"../data/data.csv\"\n",
    "schema = StructType([\n",
    "    StructField('event_name', StringType(), True),\n",
    "    StructField('item_id', IntegerType(), True), \n",
    "    StructField('timestamp', TimestampType(), True), \n",
    "    StructField('site', StringType(), True), \n",
    "    StructField('experiments', StringType(), True), \n",
    "    StructField('user_id', IntegerType(), True), \n",
    "])\n",
    "data = spark.read.schema(schema).option(\"header\",\"true\").csv(data_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformación\n",
    "\n",
    "- Se aplica la funcion parser sobre la columna experiments para volver la columna a MapType\n",
    "- Se explotan los datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_data = data.withColumn(\"parsed_experiments\",parser_udf(\"experiments\"))\n",
    "new_data = new_data.select(*schema.fieldNames(), explode(\"parsed_experiments\").alias(\"experiment_id\",\"veriant_id\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Agrupamiento\n",
    "\n",
    "- Se crea una vista para ejecutar sentencia SQL.\n",
    "- Se ejecuta SELECT GROUP BY requerido"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_data.createOrReplaceTempView(\"final_data\")\n",
    "result = spark.sql(\"\"\"\n",
    "SELECT experiment_id, veriant_id, count(*) as numero_compras\n",
    "FROM final_data\n",
    "WHERE event_name='BUY'\n",
    "GROUP BY experiment_id, veriant_id\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resultado\n",
    "\n",
    "- Se muestra resultado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 0:>                                                        (0 + 12) / 12]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------+----------+--------------+\n",
      "|experiment_id         |veriant_id|numero_compras|\n",
      "+----------------------+----------+--------------+\n",
      "|buyingflow/secure_card|4612      |31            |\n",
      "|buyingflow/user-track |6796      |1088          |\n",
      "|buyingflow/address_hub|3574      |922           |\n",
      "+----------------------+----------+--------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "result.show(truncate=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nivel1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
